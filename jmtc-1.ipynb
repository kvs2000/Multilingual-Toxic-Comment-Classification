{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  3727  100  3727    0     0  20037      0 --:--:-- --:--:-- --:--:-- 20037\r\n",
      "Updating TPU and VM. This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-dev20200325 ...\r\n",
      "Found existing installation: torch 1.4.0\r\n",
      "Uninstalling torch-1.4.0:\r\n",
      "  Successfully uninstalled torch-1.4.0\r\n",
      "Done updating TPU runtime: <Response [200]>\r\n",
      "Found existing installation: torchvision 0.5.0\r\n",
      "Uninstalling torchvision-0.5.0:\r\n",
      "  Successfully uninstalled torchvision-0.5.0\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/83.4 MiB.                                     \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/114.5 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/2.5 MiB.                                      \r\n",
      "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (1.18.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (0.18.2)\r\n",
      "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: catalyst 20.3.3 requires torchvision>=0.2.1, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: torch\r\n",
      "Successfully installed torch-1.5.0a0+d6149a7\r\n",
      "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.6+e788e5b\r\n",
      "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.14.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (5.4.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.18.2)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch->torchvision==nightly+20200325) (0.18.2)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.6.0a0+3c254fb\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  gfortran gfortran-6 libblas-common libblas-dev libblas3 libgfortran-6-dev\r\n",
      "  libgfortran3 libopenblas-base\r\n",
      "Suggested packages:\r\n",
      "  gfortran-multilib gfortran-doc gfortran-6-multilib gfortran-6-doc\r\n",
      "  libgfortran3-dbg libcoarrays-dev liblapack-doc-man liblapack-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  gfortran gfortran-6 libblas-common libblas-dev libblas3 libgfortran-6-dev\r\n",
      "  libgfortran3 libomp5 libopenblas-base libopenblas-dev\r\n",
      "0 upgraded, 10 newly installed, 0 to remove and 37 not upgraded.\r\n",
      "Need to get 15.6 MB of archives.\r\n",
      "After this operation, 122 MB of additional disk space will be used.\r\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 libgfortran3 amd64 6.3.0-18+deb9u1 [265 kB]\r\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 libgfortran-6-dev amd64 6.3.0-18+deb9u1 [299 kB]\r\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 gfortran-6 amd64 6.3.0-18+deb9u1 [6916 kB]\r\n",
      "Get:4 http://deb.debian.org/debian stretch/main amd64 gfortran amd64 4:6.3.0-4 [1356 B]\r\n",
      "Get:5 http://deb.debian.org/debian stretch/main amd64 libblas-common amd64 3.7.0-2 [14.2 kB]\r\n",
      "Get:6 http://deb.debian.org/debian stretch/main amd64 libblas3 amd64 3.7.0-2 [155 kB]\r\n",
      "Get:7 http://deb.debian.org/debian stretch/main amd64 libblas-dev amd64 3.7.0-2 [162 kB]\r\n",
      "Get:8 http://deb.debian.org/debian stretch/main amd64 libopenblas-base amd64 0.2.19-3 [3793 kB]\r\n",
      "Get:9 http://deb.debian.org/debian stretch/main amd64 libopenblas-dev amd64 0.2.19-3 [3809 kB]\r\n",
      "Get:10 http://deb.debian.org/debian stretch/main amd64 libomp5 amd64 3.9.1-1 [228 kB]\r\n",
      "Fetched 15.6 MB in 1s (9197 kB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libgfortran3:amd64.\r\n",
      "(Reading database ... 72030 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libgfortran3_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking libgfortran3:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package libgfortran-6-dev:amd64.\r\n",
      "Preparing to unpack .../1-libgfortran-6-dev_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking libgfortran-6-dev:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package gfortran-6.\r\n",
      "Preparing to unpack .../2-gfortran-6_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking gfortran-6 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package gfortran.\r\n",
      "Preparing to unpack .../3-gfortran_4%3a6.3.0-4_amd64.deb ...\r\n",
      "Unpacking gfortran (4:6.3.0-4) ...\r\n",
      "Selecting previously unselected package libblas-common.\r\n",
      "Preparing to unpack .../4-libblas-common_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas-common (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libblas3.\r\n",
      "Preparing to unpack .../5-libblas3_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas3 (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libblas-dev.\r\n",
      "Preparing to unpack .../6-libblas-dev_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas-dev (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libopenblas-base.\r\n",
      "Preparing to unpack .../7-libopenblas-base_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-base (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libopenblas-dev.\r\n",
      "Preparing to unpack .../8-libopenblas-dev_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-dev (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "Preparing to unpack .../9-libomp5_3.9.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (3.9.1-1) ...\r\n",
      "Setting up libomp5:amd64 (3.9.1-1) ...\r\n",
      "Setting up libblas-common (3.7.0-2) ...\r\n",
      "Setting up libgfortran3:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up libgfortran-6-dev:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up libblas3 (3.7.0-2) ...\r\n",
      "update-alternatives: using /usr/lib/libblas/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n",
      "Setting up libopenblas-base (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so.3 to provide /usr/lib/liblapack.so.3 (liblapack.so.3) in auto mode\r\n",
      "Setting up gfortran-6 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up gfortran (4:6.3.0-4) ...\r\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\r\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\r\n",
      "Setting up libblas-dev (3.7.0-2) ...\r\n",
      "update-alternatives: using /usr/lib/libblas/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\r\n",
      "Setting up libopenblas-dev (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so to provide /usr/lib/liblapack.so (liblapack.so) in auto mode\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, namedtuple\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "import sys\n",
    "from sklearn import metrics, model_selection\n",
    "\n",
    "import warnings\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768 * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            ids,\n",
    "            mask,\n",
    "            token_type_ids\n",
    "    ):\n",
    "        o1, o2 = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "        apool = torch.mean(o1, 1)\n",
    "        mpool, _ = torch.max(o1, 1)\n",
    "        cat = torch.cat((apool, mpool), 1)\n",
    "\n",
    "        bo = self.bert_drop(cat)\n",
    "        p2 = self.out(bo)\n",
    "        return p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDatasetTraining:\n",
    "    def __init__(self, comment_text, targets, tokenizer, max_length):\n",
    "        self.comment_text = comment_text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment_text = str(self.comment_text[item])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        padding_length = self.max_length - len(ids)\n",
    "        \n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = BERTBaseUncased(bert_path=\"../input/bert-base-multilingual-uncased/\")\n",
    "df_train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
    "df_train2 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
    "df_train_full = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\n",
    "df_train = df_train_full.sample(frac=1).reset_index(drop=True).head(200000)\n",
    "\n",
    "df_valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', \n",
    "                       usecols=[\"comment_text\", \"toxic\"])\n",
    "\n",
    "df_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run():\n",
    "    def loss_fn(outputs, targets):\n",
    "        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "        model.train()\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if bi % 10 == 0:\n",
    "                xm.master_print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "    def eval_loop_fn(data_loader, model, device):\n",
    "        model.eval()\n",
    "        fin_targets = []\n",
    "        fin_outputs = []\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            targets_np = targets.cpu().detach().numpy().tolist()\n",
    "            outputs_np = outputs.cpu().detach().numpy().tolist()\n",
    "            fin_targets.extend(targets_np)\n",
    "            fin_outputs.extend(outputs_np)    \n",
    "\n",
    "        return fin_outputs, fin_targets\n",
    "\n",
    "    \n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    EPOCHS = 2\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-multilingual-uncased/\", do_lower_case=True)\n",
    "\n",
    "    train_targets = df_train.toxic.values\n",
    "    valid_targets = df_valid.toxic.values\n",
    "\n",
    "    train_dataset = BERTDatasetTraining(\n",
    "        comment_text=df_train.comment_text.values,\n",
    "        targets=train_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          train_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=True)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDatasetTraining(\n",
    "        comment_text=df_valid.comment_text.values,\n",
    "        targets=valid_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          valid_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=False)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=16,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    model = mx.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n",
    "    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
    "    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
    "        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n",
    "\n",
    "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n",
    "        xm.save(model.state_dict(), \"model.bin\")\n",
    "        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n",
    "        xm.master_print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps = 812, world_size=8\n",
      "bi=0, loss=0.564293622970581\n",
      "bi=10, loss=0.3402901291847229\n",
      "bi=20, loss=0.31182992458343506\n",
      "bi=30, loss=0.24555177986621857\n",
      "bi=40, loss=0.3227238655090332\n",
      "bi=50, loss=0.20644468069076538\n",
      "bi=60, loss=0.24016498029232025\n",
      "bi=70, loss=0.3656253516674042\n",
      "bi=80, loss=0.3177357017993927\n",
      "bi=90, loss=0.33056870102882385\n",
      "bi=100, loss=0.2262754738330841\n",
      "bi=110, loss=0.23070532083511353\n",
      "bi=120, loss=0.27551010251045227\n",
      "bi=130, loss=0.21741044521331787\n",
      "bi=140, loss=0.25732097029685974\n",
      "bi=150, loss=0.24563854932785034\n",
      "bi=160, loss=0.20598378777503967\n",
      "bi=170, loss=0.22359442710876465\n",
      "bi=180, loss=0.19511005282402039\n",
      "bi=190, loss=0.19456921517848969\n",
      "bi=200, loss=0.177840918302536\n",
      "bi=210, loss=0.310518354177475\n",
      "bi=220, loss=0.24306896328926086\n",
      "bi=230, loss=0.19314469397068024\n",
      "bi=240, loss=0.2609172463417053\n",
      "bi=250, loss=0.2205863893032074\n",
      "bi=260, loss=0.18660475313663483\n",
      "bi=270, loss=0.22155281901359558\n",
      "bi=280, loss=0.15475620329380035\n",
      "bi=290, loss=0.22107523679733276\n",
      "bi=300, loss=0.20248094201087952\n",
      "bi=310, loss=0.2347094565629959\n",
      "bi=320, loss=0.19580812752246857\n",
      "bi=330, loss=0.31145599484443665\n",
      "bi=340, loss=0.2020861804485321\n",
      "bi=350, loss=0.25567924976348877\n",
      "bi=360, loss=0.19426017999649048\n",
      "bi=370, loss=0.19779282808303833\n",
      "bi=380, loss=0.2367938905954361\n",
      "bi=390, loss=0.22979390621185303\n",
      "bi=400, loss=0.23475228250026703\n",
      "AUC = 0.9665271287642783\n",
      "bi=0, loss=0.22128570079803467\n",
      "bi=10, loss=0.24732591211795807\n",
      "bi=20, loss=0.19312794506549835\n",
      "bi=30, loss=0.2095182240009308\n",
      "bi=40, loss=0.25475752353668213\n",
      "bi=50, loss=0.17843493819236755\n",
      "bi=60, loss=0.19765858352184296\n",
      "bi=70, loss=0.2667306065559387\n",
      "bi=80, loss=0.2744806408882141\n",
      "bi=90, loss=0.2877001166343689\n",
      "bi=100, loss=0.20654255151748657\n",
      "bi=110, loss=0.19330497086048126\n",
      "bi=120, loss=0.2689056098461151\n",
      "bi=130, loss=0.20422250032424927\n",
      "bi=140, loss=0.23388761281967163\n",
      "bi=150, loss=0.240506112575531\n",
      "bi=160, loss=0.1962650567293167\n",
      "bi=170, loss=0.1924983561038971\n",
      "bi=180, loss=0.18130157887935638\n",
      "bi=190, loss=0.1924334168434143\n",
      "bi=200, loss=0.16695137321949005\n",
      "bi=210, loss=0.27670228481292725\n",
      "bi=220, loss=0.24066156148910522\n",
      "bi=230, loss=0.1887529045343399\n",
      "bi=240, loss=0.234306201338768\n",
      "bi=250, loss=0.22465234994888306\n",
      "bi=260, loss=0.18066734075546265\n",
      "bi=270, loss=0.20513054728507996\n",
      "bi=280, loss=0.14481832087039948\n",
      "bi=290, loss=0.221808522939682\n",
      "bi=300, loss=0.17996063828468323\n",
      "bi=310, loss=0.22401273250579834\n",
      "bi=320, loss=0.17984344065189362\n",
      "bi=330, loss=0.30133381485939026\n",
      "bi=340, loss=0.19450460374355316\n",
      "bi=350, loss=0.2501836121082306\n",
      "bi=360, loss=0.1865169256925583\n",
      "bi=370, loss=0.19083650410175323\n",
      "bi=380, loss=0.22578023374080658\n",
      "bi=390, loss=0.22859501838684082\n",
      "bi=400, loss=0.21958480775356293\n",
      "AUC = 0.9840991692627207\n"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    a = _run()\n",
    "\n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
